Two Sample Inference Procedures}
### Hypothesis test for the means of two independent samples}
The procedure associated with testing a hypothesis concerning the difference between two population means is similar to that for testing a hypothesis concerning the value of one population mean.

The procedure differs mainly in that the standard error of the difference between the means is used to determine the test statistic associated with the sample result. For two tailed tests, the null hypothesis states that the population means are the same, with the alternative stating that the population means are not equal.

       
       * [Ho] : $\mu_1 = \mu_2$
       * [Ha] : $\mu_1 \neq \mu_2$
       
### Implementation with \texttt{R}}
Firstly, lets construct a second data set. In this scenario, it is not possible to score a 6, hence the dice is crooked.
(The previous fair dice data set is called $x$. This crooked dice data is labelled $y$)

<pre><code>
## Initialize variables
die2 = 1:5
N=100
y=sample(die2,N, replace=TRUE)
t.test(y)
</code></pre>


We can perform a two sample test for independent samples. In such a test the null and alternative hypotheses are as follows:

H0: True mean of $x$ is equal to true mean of $y$.

H1: True mean of $x$ is NOT equal to true mean of $y$.

An estimate for the difference of sample means, and a confidence interval for that estimate is provided in the output. The expected value under the null hypothesis does not have to be specified in this instance.


<pre><code>
## Initialize variables
die2 = 1:5
N=100
y=sample(die2,N, replace=TRUE)
t.test(y)
</code></pre>


To implement a two sample test, simply specify the names of both data sets.


<pre><code>
t.test(x,y)
</code></pre>

The output should look something like the output below. Notice the confidence interval for the difference in the means: \texttt{( -0.01885674,0.83885674 )}.
How do you interpret this output? (Hint: look at the p-value).

<pre><code>
> t.test(x,y)

        Welch Two Sample t-test

data:  x and y
t = 1.8862, df = 183.43, p-value = 0.06084
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.01885674  0.83885674
sample estimates:
mean of x mean of y
     3.39      2.98
</code></pre>
There are in fact two variants of the two sample t-test.

*  The Independent Two Sample t-test
*  The Welch Two Sample t-test
The Welch Two-Sample test (the procedure from the last segment of \texttt{R}output) does not require the assumption of equal variance in the two samples. Conversely the Independent Two-Sample test does.

To specify that the assumption of equal variance, the additional argument \textbf{\texttt{var.equal=TRUE}} is specified


<pre><code>
t.test(x,y,var.equal=TRUE)
</code></pre>



<pre><code>
> t.test(x,y,var.equal=TRUE)

        Two Sample t-test

data:  x and y
t = 1.8862, df = 198, p-value = 0.06073
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.01864727  0.83864727
sample estimates:
mean of x mean of y
     3.39      2.98
</code></pre>

### Paired t-test}
% (Population Mean between paired samples)
Two data samples aresaid to be paired (or matched) if they come from repeated observations of the same subject. Here, we assume that the data populations follow the normal distribution.

Using the paired t-test, we can obtain an interval estimate of the difference of the population means. Necessarily there must be equal numbers of elements in both sets.

The \textbf{\texttt{t.test()}} function can be used to perform paired t-tests, by making the appropriate specification:\textbf{\texttt{paired=TRUE}}.


\subsubsection{Example}
In the built-in data set named \textbf{\emph{immer}}, the barley yield in years 1931 and 1932 of the same field are recorded. In the intervening period, fertilizer treatments were applied to each field. The motivation of the study was to determine whether or not the treatment was effective.

The yield data are presented in the data frame columns $Y1$ and $Y2$.

<pre><code>
> library(MASS)         # load the MASS package
> head(immer)
   Loc  Var    Y1    Y2
1  UF   M      81.0  80.7
2  UF   S      105.4  82.3
   .....
</code></pre>
Assuming that the data in immer follows the normal distribution, find the $95\%$ confidence interval estimate of the difference between the mean barley yields.

We apply the t.test function to compute the difference in means of the matched samples. As it is a paired test, we set the ``paired" argument as TRUE.


<pre><code>
attach(immer)
t.test(Y1,Y2, paired=TRUE)
</code></pre>


<pre><code>

        Paired t-test

data:  Y1 and Y2
t = 3.324, df = 29, p-value = 0.002413
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
  6.121954 25.704713
sample estimates:
mean of the differences
               15.91333
</code></pre>
Between years 1931 and 1932 in the data set immer, the 95\% confidence interval of the difference in means of the barley yields is the interval between 6.122 and 25.705.
One can conclude that the fertilizer treatments were successful in improving the yield of barley.

\section{Non-Parametric Inference Procedures}
Nonparametric procedures were developed to be used in cases when the distribution of the variable of interest in the population is known to be not-normal, and furthermore the distribution is undetermined (hence the name nonparametric).

% MA4605 Class
Nonparametric tests are also referred to as \textbf{\emph{distribution-free}} tests. These tests have the obvious advantage of not requiring the assumption of normality or the assumption of homogeneity of variance. They compare medians rather than means and, as a result, if the data have one or two outliers, their influence is negated.

Parametric tests are preferred because, in general, for the same number of observations, they are more likely to lead to the rejection of a false hull hypothesis. That is, they have more power. This greater power stems from the fact that if the data have been collected at an interval or ratio level, information is lost in the conversion to ranked data (i.e., merely ordering the data from the lowest to the highest value).


*  Kolmogorov- Smirnov Test (\texttt{ks.test()})
*  Wilcoxon test (\texttt{wilcox.test()})


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
### Wilcoxon Mann-Whitney Test}
The Wilcoxon Mann-Whitney Test is one of the most powerful of the nonparametric tests for comparing two populations. It is used to test the null hypothesis that two populations have identical distribution functions against the alternative hypothesis that the two distribution functions differ only with respect to \textbf{\emph{location}} (i.e. median), if at all.

The Wilcoxon Mann-Whitney test does not require the assumption that the differences between the two samples are normally distributed.

In many applications, the Wilcoxon Mann-Whitney Test is used in place of the two sample t-test when the normality assumption is questionable.

This test can also be applied when the observations in a sample of data are ranks, that is, ordinal data rather than direct measurements.

%---------------------------------------------------- %
\newpage
\section{Bivariate data}
### What is Bivariate data?}

A dataset with two variables contains what is called bivariate data. For example, the heights and weights of people (i.e. for the purposes of determining the extent to which taller people weigh more)

Common bivariate statistical analyses include

*  Correlation
*  Simple Linear Regression

### Scatter Plot} A scatter plot of two variables shows the values of one variable on the Y axis and the values of the other variable on the X axis. Scatter plots are well suited for revealing the relationship between two variables.

Scatterplots can be implemented in \texttt{R} using the command \texttt{\textbf{plot()}}

Exercise: Let us construct scatter-plots for the Immer and Iris data sets.


<pre><code>
plot(immer$Y1,immer$Y2)

plot(iris[,1],iris[,3])
</code></pre>

More complex scatterplots, with better visual aesthetics, can be constructed. We will look at this more later on in the semester.
